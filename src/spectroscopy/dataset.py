import calendar
import collections
from copy import copy, deepcopy
import datetime
import inspect
import string
from uuid import uuid4
import warnings
import weakref

from dateutil import tz
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize, LogNorm
from matplotlib.pyplot import cm
import numpy as np
import tables
from scipy.stats import binned_statistic

from spectroscopy.plugins import get_registered_plugins, DatasetPluginBase
import spectroscopy.util

class ResourceIdentifier(object):
    """
    Unique identifier of any resource so it can be referred to.

    All elements of a Dataset instance have a unique id that other elements
    use to refer to it. This is called a ResourceIdentifier.

    In this class it can be any hashable object, e.g. most immutable objects
    like numbers and strings.

    :type id: str, optional
    :param id: A unique identifier of the element it refers to. It is
        not verified, that it actually is unique. The user has to take care of
        that. If no resource_id is given, uuid.uuid4() will be used to
        create one which assures uniqueness within one Python run.
        If no fixed id is provided, the ID will be built from prefix
        and a random uuid hash. The random hash part can be regenerated by the
        referred object automatically if it gets changed.
    :type prefix: str, optional
    :param prefix: An optional identifier that will be put in front of any
        automatically created resource id. The prefix will only have an effect
        if `id` is not specified (for a fixed ID string). Makes automatically
        generated resource ids more reasonable.
    :type referred_object: Python object, optional
    :param referred_object: The object this instance refers to. All instances
        created with the same resource_id will be able to access the object as
        long as at least one instance actual has a reference to it.

    .. rubric:: General Usage

    >>> ResourceIdentifier('2012-04-11--385392')
    ResourceIdentifier(id="2012-04-11--385392")
    >>> # If 'id' is not specified it will be generated automatically.
    >>> ResourceIdentifier()  # doctest: +ELLIPSIS
    ResourceIdentifier(id="...")
    >>> # Supplying a prefix will simply prefix the automatically generated ID
    >>> ResourceIdentifier(prefix='peru09')  # doctest: +ELLIPSIS
    ResourceIdentifier(id="peru09_...")

    ResourceIdentifiers can, and oftentimes should, carry a reference to the
    object they refer to. This is a weak reference which means that if the
    object gets deleted or runs out of scope, e.g. gets garbage collected, the
    reference will cease to exist.

    >>> class A(object): pass
    >>> a = A()
    >>> import sys
    >>> ref_count = sys.getrefcount(a)
    >>> res_id = ResourceIdentifier(referred_object=a)
    >>> # The reference does not change the reference count of the object.
    >>> print(ref_count == sys.getrefcount(a))
    True
    >>> # It actually is the same object.
    >>> print(a is res_id.get_referred_object())
    True
    >>> # Deleting it, or letting the garbage collector handle the object will
    >>> # invalidate the reference.
    >>> del a
    >>> print(res_id.get_referred_object())
    None

    The most powerful ability (and reason why one would want to use a resource
    identifier class in the first place) is that once a ResourceIdentifier with
    an attached referred object has been created, any other ResourceIdentifier
    instances with the same ID can retrieve that object. This works
    across all ResourceIdentifiers that have been instantiated within one
    Python run.
    This enables, e.g. the resource references between the different elements
    to work in a rather natural way.

    >>> a = A()
    >>> obj_id = id(a)
    >>> res_id = "someid"
    >>> ref_a = ResourceIdentifier(res_id)
    >>> # The object is refers to cannot be found yet. Because no instance that
    >>> # an attached object has been created so far.
    >>> print(ref_a.get_referred_object())
    None
    >>> # This instance has an attached object.
    >>> ref_b = ResourceIdentifier(res_id, referred_object=a)
    >>> ref_c = ResourceIdentifier(res_id)
    >>> # All ResourceIdentifiers will refer to the same object.
    >>> assert(id(ref_a.get_referred_object()) == obj_id)
    >>> assert(id(ref_b.get_referred_object()) == obj_id)
    >>> assert(id(ref_c.get_referred_object()) == obj_id)


    ResourceIdentifiers are considered identical if the IDs are
    the same.

    >>> # Create two different resource identifiers.
    >>> res_id_1 = ResourceIdentifier()
    >>> res_id_2 = ResourceIdentifier()
    >>> assert(res_id_1 != res_id_2)
    >>> # Equalize the IDs. NEVER do this. This is just an example.
    >>> res_id_2.id = res_id_1.id = "smi:local/abcde"
    >>> assert(res_id_1 == res_id_2)

    ResourceIdentifier instances can be used as dictionary keys.

    >>> dictionary = {}
    >>> res_id = ResourceIdentifier(id="foo")
    >>> dictionary[res_id] = "bar1"
    >>> # The same ID can still be used as a key.
    >>> dictionary["foo"] = "bar2"
    >>> items = sorted(dictionary.items(), key=lambda kv: kv[1])
    >>> for k, v in items:  # doctest: +ELLIPSIS
    ...     print repr(k), v
    ResourceIdentifier(id="foo") bar1
    ...'foo' bar2
    """
    # Class (not instance) attribute that keeps track of all resource
    # identifier throughout one Python run. Will only store weak references and
    # therefore does not interfere with the garbage collection.
    # DO NOT CHANGE THIS FROM OUTSIDE THE CLASS.
    __resource_id_weak_dict = weakref.WeakValueDictionary()
    # Use an additional dictionary to track all resource ids.
    __resource_id_tracker = collections.defaultdict(int)

    def __init__(self, oid=None, prefix=None,
                 referred_object=None):
        # Create a resource id if None is given and possibly use a prefix.
        if oid is None:
            self.fixed = False
            self._prefix = prefix
            self._uuid = str(uuid4())
        else:
            self.fixed = True
            self.id = oid
        # Append the referred object in case one is given to the class level
        # reference dictionary.
        if referred_object is not None:
            self.set_referred_object(referred_object)

        # Increment the counter for the current resource id.
        ResourceIdentifier.__resource_id_tracker[self.id] += 1

    def __del__(self):
        if self.id not in ResourceIdentifier.__resource_id_tracker:
            return
        # Decrement the resource id counter.
        ResourceIdentifier.__resource_id_tracker[self.id] -= 1
        # If below or equal to zero, delete it and also delete it from the weak
        # value dictionary.
        if ResourceIdentifier.__resource_id_tracker[self.id] <= 0:
            del ResourceIdentifier.__resource_id_tracker[self.id]
            try:
                del ResourceIdentifier.__resource_id_weak_dict[self.id]
            except KeyError:
                pass

    def get_referred_object(self):
        """
        Returns the object associated with the resource identifier.

        This works as long as at least one ResourceIdentifier with the same
        ID as this instance has an associate object.

        Will return None if no object could be found.
        """
        try:
            return ResourceIdentifier.__resource_id_weak_dict[self.id]
        except KeyError:
            return None

    def set_referred_object(self, referred_object):
        """
        Sets the object the ResourceIdentifier refers to.

        If it already a weak reference it will be used, otherwise one will be
        created. If the object is None, None will be set.

        Will also append self again to the global class level reference list so
        everything stays consistent.
        """
        # If it does not yet exists simply set it.
        if self.id not in ResourceIdentifier.__resource_id_weak_dict:
            ResourceIdentifier.__resource_id_weak_dict[self.id] = \
                referred_object
            return
        # Otherwise check if the existing element the same as the new one. If
        # it is do nothing, otherwise raise a warning and set the new object as
        # the referred object.
        if ResourceIdentifier.__resource_id_weak_dict[self.id] == \
                referred_object:
            return
        msg = "The resource identifier '%s' already exists and points to " + \
              "another object: '%s'." + \
              "It will now point to the object referred to by the new " + \
              "resource identifier."
        msg = msg % (
            self.id,
            repr(ResourceIdentifier.__resource_id_weak_dict[self.id]))
        # Always raise the warning!
        warnings.warn_explicit(msg, UserWarning, __file__,
                               inspect.currentframe().f_back.f_lineno)
        ResourceIdentifier.__resource_id_weak_dict[self.id] = \
            referred_object

    def copy(self):
        """
        Returns a copy of the ResourceIdentifier.

        >>> res_id = ResourceIdentifier()
        >>> res_id_2 = res_id.copy()
        >>> print(res_id is res_id_2)
        False
        >>> print(res_id == res_id_2)
        True
        """
        return deepcopy(self)

    @property
    def id(self):
        """
        Unique identifier of the current instance.
        """
        if self.fixed:
            return self.__dict__.get("id")
        else:
            oid = self.prefix
            if oid is not None and not oid.endswith("_"):
                oid += "_"
                oid += self.uuid
                return oid
            return self.uuid

    @id.deleter
    def id(self):
        msg = "The resource id cannot be deleted."
        raise Exception(msg)

    @id.setter
    def id(self, value):
        self.fixed = True
        # XXX: no idea why I had to add bytes for PY2 here
        if not isinstance(value, (str, bytes)):
            msg = "attribute id needs to be a string."
            raise TypeError(msg)
        self.__dict__["id"] = value

    @property
    def prefix(self):
        return self._prefix

    @prefix.deleter
    def prefix(self):
        self._prefix = ""

    @prefix.setter
    def prefix(self, value):
        if not isinstance(value, str):
            msg = "prefix id needs to be a string."
            raise TypeError(msg)
        self._prefix = value

    @property
    def uuid(self):
        return self._uuid

    @uuid.deleter
    def uuid(self):
        """
        Deleting is uuid hash is forbidden and will not work.
        """
        msg = "The uuid cannot be deleted."
        raise Exception(msg)

    @uuid.setter
    def uuid(self, value):  # @UnusedVariable
        """
        Setting is uuid hash is forbidden and will not work.
        """
        msg = "The uuid cannot be set manually."
        raise Exception(msg)

    @property
    def resource_id(self):
        return self.id

    @resource_id.deleter
    def resource_id(self):
        del self.id

    @resource_id.setter
    def resource_id(self, value):
        self.id = value

    def __str__(self):
        return self.id

    def _repr_pretty_(self, p, cycle):
        p.text(str(self))

    def __repr__(self):
        return 'ResourceIdentifier(id="%s")' % self.id

    def __eq__(self, other):
        if self.id == other:
            return True
        if not isinstance(other, ResourceIdentifier):
            return False
        if self.id == other.id:
            return True
        return False

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        """
        Uses the same hash as the resource id. This means that class instances
        can be used in dictionaries and other hashed types.

        Both the object and it's id can still be independently used as
        dictionary keys.
        """
        # "Salt" the hash with a string so the hash of the object and a
        # string identical to the id can both be used as individual
        # dictionary keys.
        return hash("RESOURCE_ID") + self.id.__hash__()

    def regenerate_uuid(self):
        """
        Regenerates the uuid part of the ID. Does nothing for resource
        identifiers with a user-set, fixed id.
        """
        self._uuid = str(uuid4())


class RetVal(object):
    """
    Wrapper to make tables.array.Array read only.
    """

    def __init__(self, wrapped_object):
        self.__dict__['_wrapped_object'] = wrapped_object
        attributes = dir(wrapped_object)
        for attr in attributes:
            if hasattr(self, attr):
                continue
            self.__dict__[attr] = attr

    def __setitem__(self, key, value):
        raise AttributeError('Data type is read only.')

    def __setslice__(self, key, value):
        raise AttributeError('Data type is read only.')

    def __setattr__(self, key, value):
        raise AttributeError('Data type is read only.')

    def __getattribute__(self, key):
        if key in ['_wrapped_object', '__dict__', '__class__']:
            return object.__getattribute__(self, key)
        return getattr(self._wrapped_object, key)

    def __getitem__(self,key):
        return self._wrapped_object.__getitem__(key)

    def __str__(self):
        return self._wrapped_object.__str__()


class H5Set(set):
    """
    An hdf5 set class for tags.
    """

    def __init__(self, h5node):
        self.h5node = h5node

    def add(self, val):
        f = self.h5node._v_file
        if val in self:
            return
        set.add(self, val)
        try:
            f.create_earray(
                '/tags', val, tables.StringAtom(itemsize=60), (0,))
        except:
            pass
        ea = f.root.tags._v_children[val]
        found = False
        for i in range(ea.nrows):
            if ea[i] == '':
                ea[i] = np.array(
                    self.h5node._v_name['resource_id'], dtype='S60')
                found = True
                break
        if not found:
            ea.append(
                np.array([self.h5node._v_name['resource_id']], dtype='S60'))

    def remove(self, val):
        f = self.h5node._v_file
        set.remove(self, val)
        ea = f.root.tags._v_children[val]
        ea[np.where(ea[:] == self.h5node._v_name['resource_id'].encode())] = np.array(
            [''], dtype='S60')
        if np.all(ea[:] == np.array('', dtype='S60')):
            f.remove_node('/tags/' + val)

    def pop(self):
        val = set.pop(self)
        self.remove(val)
        return val

    def discard(self, val):
        try:
            self.remove(val)
        except KeyError:
            pass

    def clear(self):
        while True:
            try:
                self.pop()
            except:
                break

    def update(self, vals):
        for v in vals:
            self.add(v)

    def difference_update(self, vals):
        for v in vals:
            self.discard(v)


def _class_factory(class_name, class_type='base', class_attributes=[], class_references=[]):
    """
    Class factory to unify the creation of all the types in the datamodel.
    """

    class DataElementBase(object):
        """
        A base class with type checking for non-extendable elements in the datamodel.
        """
        # Assign properties of the data element including the expected data types
        _properties = []
        for item in class_attributes:
            _properties.append((item[0], item[1]))
        _property_keys = [_i[0] for _i in _properties]
        _property_dict = {}
        for key, value in _properties:
            _property_dict[key] = value

        # Assign references to other elements in the datamodel
        _references = []
        for item in class_references:
            _references.append((item[0],item[1]))
        _reference_keys = [_i[0] for _i in _references] 
        _reference_dict = {}
        for key, value in _references:
            _reference_dict[key] = value 

        def __init__(self, h5node, data_buffer=None):
            # Set the parent HDF5 group after type checking
            if (type(h5node) is not tables.group.Group):
                raise Exception("%s and %s are incompatible types." %
                                (type(h5node), tables.group.Group))
            self.__dict__['_root'] = h5node
            self.__dict__['_tags'] = H5Set(h5node)
            # Every time a new object is created it gets a new resource ID
            self.__dict__['_resource_id'] = ResourceIdentifier(oid=h5node._v_name,
                                                              referred_object=self)
            if data_buffer is not None:
                dtp = []
                vals = {}
                for key, prop_type in self._property_dict.iteritems():
                    val = getattr(data_buffer,key,None)
                    if val is None:
                        continue
                    if prop_type[0] == datetime.datetime:
                        datestring = val.isoformat()
                        vals[key] = datestring
                        dtp.append((key,np.dtype('S'+str(len(datestring))),()))
                    else: 
                        vals[key] = val
                        dtp.append((key,val.dtype,val.shape))
                for key in self._reference_keys:
                    val = getattr(data_buffer,key,None)
                    if val is None:
                        continue
                    vals[key] = val
                    dtp.append((key,np.dtype('S'+str(len(val))),()))            
                f = h5node._v_file
                table = f.create_table(h5node,'data', np.dtype(dtp))
                entry = table.row
                for key,val in vals.iteritems():
                    entry[key]  = val
                entry.append()
                table.flush() 

        @property
        def tags(self):
            return self._tags

        def __setattr__(self, name, value):
            # Raise an exception if not a property or attribute
            raise AttributeError(
                '{} attributes are read only. Use append method instead.'.format(type(self).__name__))

        def __getattr__(self, name):
           if name in self._property_keys:
                table = getattr(self._root,'data')
                if issubclass(self._property_dict[name][0], ResourceIdentifier):
                    val = ResourceIdentifier(table[0][name]).get_referred_object()
                elif issubclass(self._property_dict[name][0], np.ndarray):
                    val = RetVal(getattr(table.cols,name))
                elif issubclass(self._property_dict[name][0], datetime.datetime):
                    val = parse_iso_8601(getattr(table.cols,name))
                else:
                    val = copy(getattr(table.cols,name)[0])
                return val
           if name in self._reference_keys:
                table = getattr(self._root,'data')
                return ResourceIdentifier(table[0][name]).get_referred_object()
                

    class ExpandableDataElement(DataElementBase):
        """
        A base class with type checking for extendable elements in the datamodel.
        """

        def append(self):
            pass

    class DataElementBuffer(object):
        # Every element has to have an ID and a reference to the plugin
        # root node
        _properties = []
        for item in class_attributes:
            _properties.append((item[0], item[1]))
        _property_keys = [_i[0] for _i in _properties]
        _property_dict = {}
        for key, value in _properties:
            _property_dict[key] = value

        # Assign references to other elements in the datamodel
        _references = []
        for item in class_references:
            _references.append((item[0],item[1]))
        _reference_keys = [_i[0] for _i in _references] 
        _reference_dict = {}
        for key, value in _references:
            _reference_dict[key] = value 

        def __init__(self, **kwargs):
            # Set all property values to None or the kwarg value.
            for key, _ in self._properties:
                value = kwargs.get(key, None)
                setattr(self, key, value)
            for key in self._reference_keys:
                value = kwargs.get(key,None)
                setattr(self, key, value)

        def __setattr__(self, name, value):
            try:
                attrib_type = self._property_dict[name]
            except KeyError:
                try:
                    attrib_type = self._reference_dict[name]
                except KeyError:
                    raise Exception(
                        "%s is not a property or reference of class %s" %
                        (name, type(self).__name__))
            # If the value is None or already the correct type just set it.
            if (value is not None) and (type(value) not in attrib_type):
                msg = "{:s} is not in the list of compatible types: {}"
                raise Exception(msg.format(type(value), attrib_type))
            if value is not None:
                if name in self._reference_keys:
                    value = str(getattr(value,'_resource_id'))
                else:
                    if attrib_type[0] == np.ndarray:
                        value = np.array(value)
                    elif self._property_dict[name][0] == datetime.datetime:
                        value = spectroscopy.util.parse_iso_8601(value)
                    else:
                        value = attrib_type[0](value)
            self.__dict__[name] = value

    if class_type == 'base':
        base_class = DataElementBase
    elif class_type == 'extendable':
        base_class = ExpandableDataElement
    elif class_type == 'buffer':
        base_class = DataElementBuffer
    # Set the class type name.
    setattr(base_class, "__name__", class_name)
    return base_class


class _DataElementWriter(object):
    
    def __init__(self):
        pass
    
    


class Dataset(object):
    """
    This class is a container for all data describing a spectroscopy analysis
    from the raw measurements, over instruments and information on gas plumes
    to the final gas flux results.

    :type preferredFluxIDs: list
    :param preferredFluxIDs: IDs of the best/final flux estimate. As a dataset
        can contain analyses from different targets there can be more than one
        preferred flux estimate.
    :type spectra: list
    :param spectra: List of all spectra that are part of the dataset.
    :type instruments: list
    :param instruments: List of all instruments that are part of the dataset.
    :type retrievals: list
    :param retrievals: List of all retrievals that are part of the dataset.
    :type plumevelocities: list
    :param plumevelocities: List of all plume velocities that are part of the
        dataset.
    :type targets: list
    :param targets: List of all target plumes that are part of the dataset.
    :type flux: list
    :param flux: List of all flux estimates that are part of the dataset.
    """

    def __init__(self, filename, mode):
        self.preferred_fluxes = []
        self.fluxes = []
        self.methods = []
        self.gas_flows = []
        self.concentrations = []
        self.raw_data = []
        self.instruments = []
        self.targets = []
        self.raw_data_types = []
        self.data_quality_types = []
        self._rids = {}
        self._f = tables.open_file(filename, mode)
        self.func_table = {'TargetBuffer': self._new_target,
                           'RawDataBuffer': self._new_raw_data
                          }

    def __del__(self):
        self._f.close()

    def _new_raw_data(self, rb):
        rid = ResourceIdentifier()
        try:
            self._f.create_group('/','RawData')
        except tables.NodeError:
            pass
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            self._f.create_group('/RawData',str(rid))

        return _RawData(getattr(self._f.root.RawData,str(rid)),rb)

    def _new_target(self,tb):
        """
        Create a new target object entry in the HDF5 file.
        """
        rid = ResourceIdentifier()
        try:
            self._f.create_group('/','Target')
        except tables.NodeError:
            pass
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            self._f.create_group('/Target',str(rid))

        return _Target(getattr(self._f.root.Target,str(rid)),tb)
        
    def new(self, data_buffer):
        """
        Create a new entry in the HDF5 file from the given data buffer.
        """
        return self.func_table[type(data_buffer).__name__](data_buffer)

    def plot(self, toplot='retrievals', savefig=None, **kargs):
        """
        Provide overview plots for data contained in a dataset.

        :type toplot: str
        :param toplot: Choose the datatype to plot.

        Parameters specific to `retrievals` contour plots:

        :type log: bool
        :param log: Turn on logarithmic colour scales.
        :type cmap_name: str
        :param cmap_name: The name of the matplotlib colour scale to use.
        :type angle_bins: :class:`numpy.ndarray`
        :param angle_bins: Define the bins onto which the angles of the
            retrievals are discretized to.
        :type ncontours: int
        :param ncontours: Number of contours used in the contour plot.

        """

        if toplot.lower() == 'concentrations':
            cmap_name = kargs.get('cmap_name', 'RdBu_r')
            log = kargs.get('log', False)
            angle_bins = kargs.get('angle_bins', np.arange(0, 180, 1.0))
            ncontours = kargs.get('ncontours', 100)
            ts = kargs.get('timeshift', 0.0) * 60. * 60.
            cmap = cm.get_cmap(cmap_name)
            # dicretize all retrievals onto a grid to show a daily plot
            rts = self.concentrations
            nretrieval = len(rts)
            m = np.zeros((nretrieval, angle_bins.size - 1))

            # first sort retrievals based on start time
            def mycmp(r1, r2):
                s1 = r1.rawdata_id.get_referred_object()
                t1 = s1.time[r1.rawdata_indices].min()
                s2 = r2.rawdata_id.get_referred_object()
                t2 = s2.time[r2.rawdata_indices].min()
                if t1 < t2:
                    return -1
                if t1 == t2:
                    return 0
                if t1 > t2:
                    return 1
            rts.sort(cmp=mycmp)

            for i, _r in enumerate(rts):
                _s = _r.rawdata_id.get_referred_object()
                _angle = _s.angle[_r.rawdata_indices]
                _so2 = _r.sca
                _so2_binned = binned_statistic(
                    _angle, _so2, 'mean', angle_bins)
                m[i, :] = _so2_binned.statistic

            fig = plt.figure()
            if log:
                z = np.where(m > 0.0, m, 0.1)
                plt.contourf(range(nretrieval), angle_bins[1:], z.T, ncontours,
                             norm=LogNorm(z.min(), z.max()), cmap=cmap)
            else:
                z = np.ma.masked_invalid(m)
                plt.contourf(range(nretrieval), angle_bins[1:], m.T, ncontours,
                             norm=Normalize(z.min(), z.max()), cmap=cmap)
            new_labels = []
            new_ticks = []
            ymin = angle_bins[-1]
            ymax = angle_bins[0]
            for _xt in plt.xticks()[0]:
                try:
                    _r = rts[int(_xt)]
                    _s = _r.spectra_id.get_referred_object()
                    _a = _s.angle[_r.rawdata_indices]
                    ymin = min(_a.min(), ymin)
                    ymax = max(_a.max(), ymax)
                    dt = datetime.datetime.fromtimestamp(
                        _s.time[_r.rawdata_indices].min(), tz=tz.gettz('UTC'))
                    dt += datetime.timedelta(seconds=ts)
                    new_labels.append(dt.strftime("%Y-%m-%d %H:%M"))
                    new_ticks.append(_xt)
                except IndexError:
                    continue
            plt.xticks(new_ticks, new_labels, rotation=30,
                       horizontalalignment='right')
            cb = plt.colorbar()
            cb.set_label('Slant column amount SO2 [ppm m]')
            plt.ylim(ymin, ymax)
            plt.ylabel(r'Angle [$\circ$]')
            if savefig is not None:
                plt.savefig(
                    savefig, bbox_inches='tight', dpi=300, format='png')
            return fig

        else:
            print 'Plotting %s has not been implemented yet' % toplot

            
__RawDataType = _class_factory('__RawDataType', 'base',
                               class_attributes=[('name', (np.str_,str)),
                                                 ('acquisition', (np.str_,str))])

__RawDataTypeBuffer = _class_factory(
    '__RawDataTypeBuffer', 'buffer', __RawDataType._properties)


class RawDataTypeBuffer(__RawDataTypeBuffer):
    """
    """


class _RawDataType(__RawDataType):
    """
    """




__Target = _class_factory('__Target', 'base',
    class_attributes=[
        ('target_id', (np.str_,str)),
        ('name', (np.str_,str)),
        ('position', (np.ndarray,list,tuple)),
        ('position_error', (np.ndarray,list,tuple)),
        ('description', (np.str_,str)),
        ('creation_time', (np.str_,str))])

__TargetBuffer = _class_factory(
    '__TargetBuffer', 'buffer', __Target._properties)


class TargetBuffer(__TargetBuffer):
    """
    """


class _Target(__Target):
    """
    This class describes a target plume.

    :type position: :class:`numpy.ndarray`
    :param position: Position of a plume in decimal degrees for longitude and
        latitude and m above sea level for elevation.
    :type description: str, optional
    :param description: Any additional information on the plume that may be
        relevant.
    """

__Instrument = _class_factory('__Instrument', 'base',
                              class_attributes=[('no_bits', (np.int32,int)),
                                                ('sensor_id', (np.str_,str)),
                                                ('location', (np.str_,str)),
                                                ('type', (np.str_,str)),
                                                ('description', (np.str_,str))])

__InstrumentBuffer = _class_factory(
    '__InstrumentBuffer', 'buffer', __Instrument._properties)


class InstrumentBuffer(__InstrumentBuffer):
    """
    """


class _Instrument(__Instrument):
    """
    This class describes the spectrometer.

    :type no_bits: int
    :param no_bits: The number of bits used by the analog-to-digital converter.
    :type type: str
    :param type: The spectrometer type (e.g. FlySpec, MiniDOAS, etc.)
    :type spectrometer_ID: str
    :param spectrometer_ID: The spectrometer's ID as given by the manufacturer.
    :type description: str, optional
    :param description: Any additional information on the instrument that may
        be relevant.
    """


__RawData = _class_factory('__RawData', 'extendable',
                           class_attributes=[('inc_angle', (float,)),
                                             ('inc_angle_error', (float,)),
                                             ('bearing', (float,)),
                                             ('bearing_error', (float,)),
                                             ('position', (float,)),
                                             ('position_error', (float,)),
                                             ('path_length', (float,)),
                                             ('path_length_error', (float,)),
                                             ('d_var', (np.ndarray,list,tuple)),
                                             ('ind_var', (np.ndarray,list,tuple)),
                                             ('datetime', (datetime.datetime,np.str_,str)),
                                             ('data_quality', (np.ndarray,list,tuple)),
                                             ('data_quality_type', (np.ndarray,list,tuple)),
                                             ('integration_time', (float,)),
                                             ('no_averages', (float,)),
                                             ('temperature', (float,)),
                                             ('creation_time', (datetime.datetime,np.str_,str)),
                                             ('modification_time', (datetime.datetime,np.str_,str)),
                                             ('user_notes', (np.str_,str))],
                          class_references=[('instrument',(_Instrument,)),
                                            ('target',(_Target,)),
                                            ('type',(_RawDataType,))])

__RawDataBuffer = _class_factory(
    '__RawDataBuffer', 'buffer', __RawData._properties, __RawData._references)


class RawDataBuffer(__RawDataBuffer):
    """
    """


class _RawData(__RawData):
    """
    This class describes spectra as retrieved from a spectrometer.

    A spectra class can contain multiple scans or multiple transects. Spectra
    instances can be extended but can only be deleted as a whole. Modifying
    contents of a Spectra instance will result in a new Spectra instance with a
    modified ID. Every :class:`spectroscopy.dataset.Spectra` instance has to
    be part of a :class:`spectroscopy.dataset.Dataset` instance.

    :type instrument_id: str
    :param instrument_id: The ID of the :class:`Instrument` recording the
        spectra.
    :type target_id: str
    :param target_id: The ID of the target plume.
    :type angle: :class:`numpy.ndarray`
    :param angle: Angles corresponding to the spectra in a scan in degrees from
        the horizontal. The number of angles has to be equal to the number of
        recorded spectra. For a transect all angles would typically be the
        same, e.g. 90.0 if the spectrometer was pointing up.
    :type angle_error: :class:`numpy.ndarray`
    :param angle_error: Uncertainty for every angle.
    :type bearing: float
    :param bearing: Bearing of the scan plane in degrees from grid north.
    :type bearing_error: float
    :param bearing_error: Bearing uncertainty.
    :type position: :class:`numpy.ndarray`
    :param position: The position of the spectrometer in decimal longitude,
        latitude, and elevation in m above sea level. For a scan this would be
        a 1x3 array, for a transect it would be an Sx3 matrix, where S is the
        number of recorded spectra.
    :type position_error: :class:`numpy.ndarray`
    :param position_error: Position uncertainty.
    :type counts: :class:`numpy.ndarray`
    :param counts: The raw spectra as returned from the spectrometer. It is a
        matrix of dimension S x W where S is the number of spectra and W the
        number of wavelengths determined by the spectrometer.
    :type wavelengths: :class:`numpy.ndarray`
    :param wavelengths: A 1 x W array containing the wavelengths for the
        spectra.
    :type time: :class:`numpy.ndarray`
    :param time: Unix timestamp for the measurement time of every spectrum.
    :type integration_time: :class:`numpy.ndarray`
    :param integration_time: Integration time for every spectrum.
    :type no_averages: :class:`numpy.ndarray`
    :param no_averages: Number of averages for every spectrum.
    :type creation_time: float
    :param creation_time: Unix timestamp for the creation time of the
        :class:`Spectra` instance.
    :type modification_time: float
    :param modification_time: Unix timestamp for the latest modification time
        of the :class:`Spectra` instance.
    :type type: int
    :param type: The spectrum type {0:dark, 1:offset, 2:measurement,
        3:background, 4:other}
    :type user_notes: str, optional
    :param user_notes: Additional notes.
    """

__GasFlow = _class_factory('__GasFlow', 'extendable',
                           class_attributes=[('vx', (np.ndarray,list,tuple)),
                                             ('vx_error', (np.ndarray,list,tuple)),
                                             ('vy', (np.ndarray,list,tuple)),
                                             ('vy_error', (np.ndarray,list,tuple)),
                                             ('vz', (np.ndarray,list,tuple)),
                                             ('vz_error', (np.ndarray,list,tuple)),
                                             ('unit', (np.str_,str)),
                                             ('position', (np.ndarray,list,tuple)),
                                             ('position_error', (np.ndarray,list,tuple)),
                                             ('grid_bearing', (float,)),
                                             ('grid_increments', (np.ndarray,list,tuple)),
                                             ('pressure', (np.ndarray,list,tuple)),
                                             ('temperature', (np.ndarray,list,tuple)),
                                             ('datetime', (np.ndarray,list,tuple)),
                                             ('creation_time', (float,)),
                                             ('modification_time', (float,)),
                                             ('user_notes', (np.str_,str))],
                          class_references=[('methods',(np.ndarray,list,tuple))])

__GasFlowBuffer = _class_factory(
    '__GasFlowBuffer', 'buffer', __GasFlow._properties, __GasFlow._references)


class GasFlowBuffer(__GasFlowBuffer):
    """
    """


class _GasFlow(__GasFlow):
    """
    This class describes the plumevelocity, which can be either based on
    meteorological data or direct measurements (e.g. from motion tracking or
    weather stations).

    It can store anything from values at a particular point to 4-dimensional
    models of wind speed or plume velocity. Grids are assumed to be
    right-handed Cartesian coordinate systems with uniform grid point spacing
    along any direction. In the following X, Y, and Z are assumed to be the
    spatial dimensions of the grid and T is the number of time steps. For a
    point values X, Y, and Z would be equal to 1.

    :type vx: :class:`numpy.ndarray`
    :param vx: x-component of the wind speed or plume velocity vector in m/s.
    :type vx_error: :class:`numpy.ndarray`
    :param vx_error: Uncertainty in the x-component of the wind speed or plume
        velocity vector.
    :type vy: :class:`numpy.ndarray`
    :param vy: y-component of the wind speed or plume velocity vector.
    :type vy_error: :class:`numpy.ndarray`
    :param vy_error: Uncertainty in the y-component of the wind speed or plume
        velocity vector.
    :type vz: :class:`numpy.ndarray`
    :param vz: z-component of the wind speed or plume velocity vector.
    :type vz_error: :class:`numpy.ndarray`
    :param vz_error: Uncertainty in the z-component of the wind speed or plume
        velocity vector.
    :type position: :class:`numpy.ndarray`
    :param position: Grid origin or position of a weather station in decimal
        degrees for longitude and latitude and m above sea level for elevation.
    :type position_error: :class:`numpy.ndarray`
    :param position_error: Uncertainty in grid origin or weather station
        position.
    :type grid_bearing: float
    :param grid_bearing: Angle of the X-axis from grid north in decimal
        degrees.
    :type grid_increments: :class:`numpy.ndarray`, optional
    :param grid_increments: Grid increments in meters along the X-, Y-, and
        Z-component. This parameter is only required if grid values are used.
    :type time: :class:`numpy.ndarray`
    :param time: Time axis of unix timestamps.
    :type creation_time: float
    :param creation_time: Unix timestamp for the creation time of the
        :class:`Plumevelocity` instance.
    :type modification_time: float
    :param modification_time: Unix timestamp for the latest modification time
        of the :class:`Plumevelocity` instance.
    :type description: str, optional
    :param description: Any additional information that may be relevant.
    """

    def get_velocity(self, longitude, latitude, height, date):
        """
        Find plume velocity that is closest to the given location and time.

        :type longitude: float
        :param longitude: Longitude of the point of interest.
        :type latitude: float
        :param latitude: Latitude of the point of interest.
        :type altitude: float
        :param altitude: Altitude in meters of the point of interest.
        :type date: str
        :param date: Date of interest formatted according to the ISO8601
            standard.
        """
        from scipy.spatial import KDTree
        from spectroscopy.util import parse_iso_8601
        _d = parse_iso_8601(date)
        _ts = calendar.timegm((_d.utctimetuple()))
        # find nearest point
        _t = np.atleast_2d(self.time).T
        # TODO: this needs to be changed to account for regular and irregular
        # grids
        a = np.append(self.position, _t, axis=1)
        tree = KDTree(a, leafsize=a.shape[0] + 1)
        point = [longitude, latitude, height, _ts]
        distances, ndx = tree.query([point], k=1)
        vx = self.vx[ndx[0]]
        vx_error = self.vx_error[ndx[0]]
        vy = self.vy[ndx[0]]
        vy_error = self.vy_error[ndx[0]]
        vz = self.vz[ndx[0]]
        vz_error = self.vz_error[ndx[0]]
        time = self.time[ndx[0]]
        lon, lat, hght = self.position[ndx[0], :]
        return (lon, lat, hght, time, vx, vx_error, vy, vy_error, vz, vz_error)

__Method = _class_factory('__Method', 'base',
                          class_attributes=[('name',(np.str_,str)),
                                           ('description',(np.str_,str)),
                                           ('settings',(np.str_,str)),
                                           ('reference',(np.str_,str)),
                                           ('creation_time',(np.str_,str))],
                          class_references=[('raw_data',_RawData)])

__MethodBuffer = _class_factory('__MethodBuffer', 'buffer',
                                __Method._properties, __Method._references)

class MethodBuffer(__MethodBuffer):
    """
    Description of the analysis method.
    """

class _Method(__Method):
    """
    """

__Concentration = _class_factory('__Concentration', 'extendable',
                                 class_attributes=[('rawdata_indices', (slice,np.ndarray,list,tuple)),
                                                   ('gas_species', (np.str_, str)),
                                                   ('value', (np.ndarray,list,tuple)),
                                                   ('value_error', (np.ndarray,list,tuple)),
                                                   ('unit', (np.str_,str)),
                                                   ('analyst_contact', (np.str_,str)),
                                                   ('creation_time', (np.str_,str)),
                                                   ('modification_time', (np.str_,str)),
                                                   ('user_notes', (np.str_,str))],
                                class_references=[('method',(_Method,)),
                                                  ('gasflow',(_GasFlow,)),
                                                  ('rawdata',(_RawData,))])


__ConcentrationBuffer = _class_factory(
    '__ConcentrationBuffer', 'buffer', __Concentration._properties)


class ConcentrationBuffer(__ConcentrationBuffer):
    """
    """


class _Concentration(__Concentration):
    """
    This class describes retrievals based on a specific set of spectra.

    Like spectra, retrievals can be extended but only deleted as a whole and
    only if the Retrievals instance is not linked to.

    :type spectra_id: str
    :param spectra_id: ID of the :class:`Spectra` instance the retrievals are
        based on.
    :type slice: slice
    :param slice: Start and end index of a slice of spectra used to compute the
        retrieval.
    :type type: str
    :param type: The type of the retrieval, e.g. FlySpec, DOAS, etc.
    :type gas_species: str
    :param gas_species: The gas species that is targeted, e.g. SO2, BrO, etc.
    :type sca: :class:`numpy.ndarray`
    :param sca: The slant column amount in molec/(cm*cm). The dimension of sca
       is determined by the slice parameter.
    :type sca_error: :class:`numpy.ndarray`
    :param sca_error: Uncertainty of the slant column amount.
    :type software_name: str
    :param software_name: Name of the software used to compute the retrievals.
    :type software_version: str
    :param software_version: Version of the software used to compute the
        retrievals.
    :type software_settings: str
    :param software_settings: A compressed json string describing the retrieval
        parameters, e.g. reference spectra, instrument line shape, etc.
    :type analyst_name: str
    :param analyst_name: Name of the person who ran the analysis software.
    :param creation_time: Unix timestamp for the creation time of the
        :class:`Retrievals` instance.
    :type modification_time: float
    :param modification_time: Unix timestamp for the latest modification time
        of the :class:`Retrievals` instance.
    :type user_notes: str, optional
    :param user_notes: Additional notes.
    """


__Flux = _class_factory('__Flux', 'extendable',
                        class_attributes=[('concentration', ResourceIdentifier),
                                          ('method', ResourceIdentifier),
                                          ('concentration_indices', list),
                                          ('gasflow',
                                           ResourceIdentifier),
                                          ('value', np.ndarray),
                                          ('value_error', np.ndarray),
                                          ('analyst_contact', str),
                                          ('creation_time', float),
                                          ('modification_time', float),
                                          ('user_notes', str)])

__FluxBuffer = _class_factory('__FluxBuffer', 'buffer', __Flux._properties)


class FluxBuffer(__FluxBuffer):
    """
    """


class _Flux(__Flux):
    """
    This class stores flux estimates based on a specific set of retrievals.

    Like spectra and retrievals, Flux instances can be extended but only
    deleted as a whole and only if the Retrievals instance is not linked to.

    :type retrieval_id: str
    :param retrieval_id: ID of the :class:`Retrievals` instance the flux
        estimates are based on.
    :type slice: slice
    :param slice: Start and end index of a slice of retrievals used to compute
        the flux, which is equivalent to n numbers of complete scans.
    :type plumevelocity_id: str
    :param plumevelocity_id: ID of the :class:`Plumevelocity` instance the flux
        estimates are based on.
    :type time: :class:`numpy.ndarray`
    :param time: An array of time stamps (Unix timestamp in ms) of length T2 to
        which flux estimates are assigned to.
    :type flux: :class:`numpy.ndarray`
    :param flux: An 1 x T2 array of flux estimates.
    :type flux_error: :class:`numpy.ndarray`
    :param flux_error: Uncertainties in the flux estimates.
    :type software_name: str
    :param software_name: Name of the software used to compute the flux.
    :type software_version: str
    :param software_version: Version of the software used to compute the flux.
    :type software_settings: str
    :param software_settings: A compressed json string describing the flux
        parameters.
    :type analyst_name: str
    :param analyst_name: Name of the person who ran the analysis software.
    :param creation_time: Unix timestamp for the creation time of the
        :class:`Flux` instance.
    :type modification_time: float
    :param modification_time: Unix timestamp for the latest modification time
        of the :class:`Flux` instance.
    :type user_notes: str, optional
    :param user_notes: Additional notes.
    """

__PreferredFlux = _class_factory('__PreferredFlux', 'extendable',
                                 class_attributes=[('fluxes', list),
                                                   ('flux_indices', list),
                                                   ('method',
                                                    ResourceIdentifier),
                                                   ('value', np.ndarray),
                                                   ('value_error', np.ndarray),
                                                   ('datetime', np.ndarray),
                                                   ('analyst_contact', str),
                                                   ('creation_time', float),
                                                   ('modification_time',
                                                    float),
                                                   ('user_notes', str)])

__PreferredFluxBuffer = _class_factory(
    '__PreferredFluxBuffer', 'buffer', __PreferredFlux._properties)


class PreferredFluxBuffer(__PreferredFluxBuffer):
    """
    """


class _PreferredFlux(__PreferredFlux):
    """
    """

if __name__ == '__main__':
    import doctest
    doctest.testmod(exclude_empty=True)
